{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab56891-d0fb-423d-8535-afd00cf93acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5db5af0-bfdf-4dc6-8348-3d2c2bbc7d6b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Surface-level Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ab3d06-5d85-40da-bb52-1d7851907119",
   "metadata": {},
   "source": [
    "## BLEU (Bilingual Evaluation Understudy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "384db2b4-9c2d-4d01-a66f-16ac5d8d6e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from math import exp, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f37bb018-b0cc-4ed0-8110-90d595e7b271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bleu(reference_tokens, hypothesis_tokens, weights=(0.5, 0.5)):\n",
    "    \"\"\"\n",
    "    Compute sentence-level BLEU score with smoothing.\n",
    "    \n",
    "    Args:\n",
    "        reference_tokens (list): Reference sentence tokenized (e.g., ['‡§∞‡§æ‡§Æ‡§É', '‡§µ‡§®‡§Ç', '‡§ó‡§ö‡•ç‡§õ‡§§‡§ø'])\n",
    "        hypothesis_tokens (list): Hypothesis sentence tokenized\n",
    "        weights (tuple): Weights for n-grams (e.g., (0.5, 0.5) for bigram)\n",
    "    \n",
    "    Returns:\n",
    "        float: BLEU score\n",
    "    \"\"\"\n",
    "    smoothie = SmoothingFunction().method1\n",
    "    score = sentence_bleu([reference_tokens], hypothesis_tokens, weights=weights, smoothing_function=smoothie)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0847cef-b26d-4910-a82a-8f5b7fe00f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.707\n"
     ]
    }
   ],
   "source": [
    "# Reference and Hypothesis tokens (Sanskrit)\n",
    "reference = ['‡§∞‡§æ‡§Æ‡§É', '‡§µ‡§®‡§Ç', '‡§ó‡§ö‡•ç‡§õ‡§§‡§ø']\n",
    "hypothesis = ['‡§ó‡§ö‡•ç‡§õ‡§§‡§ø', '‡§∞‡§æ‡§Æ‡§É', '‡§µ‡§®‡§Ç']\n",
    "\n",
    "# Compute BLEU with bigram weights (unigram + bigram)\n",
    "score = compute_bleu(reference, hypothesis, weights=(0.5, 0.5))\n",
    "\n",
    "print(f\"BLEU Score: {score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa6f128-a155-4ed6-bd79-74ce13820a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "785a1e60-613d-4698-ba00-5f6821eb42f3",
   "metadata": {},
   "source": [
    "## METEOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd5f74d9-50c2-4ecd-ba8c-0cb5aaeaecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_meteor(reference_tokens, hypothesis_tokens, gamma=0.5, beta=3.0):\n",
    "    \"\"\"\n",
    "    Compute METEOR score for a single reference-hypothesis pair using unigram overlap.\n",
    "\n",
    "    Args:\n",
    "        reference_tokens (list): Tokenized reference sentence\n",
    "        hypothesis_tokens (list): Tokenized hypothesis sentence\n",
    "        gamma (float): Penalty weight (default 0.5)\n",
    "        beta (float): Penalty exponent (default 3.0)\n",
    "\n",
    "    Returns:\n",
    "        float: METEOR score\n",
    "    \"\"\"\n",
    "    # Step 1: Match unigrams\n",
    "    matches = [token for token in hypothesis_tokens if token in reference_tokens]\n",
    "    m = len(matches)\n",
    "    if m == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Step 2: Compute Precision and Recall\n",
    "    precision = m / len(hypothesis_tokens)\n",
    "    recall = m / len(reference_tokens)\n",
    "\n",
    "    # Step 3: Compute F-mean\n",
    "    f_mean = (10 * precision * recall) / (9 * precision + recall)\n",
    "\n",
    "    # Step 4: Estimate Chunks (simplified for example)\n",
    "    # Chunks are counted as groups of matched tokens appearing in the same relative order\n",
    "    def count_chunks(ref, hyp):\n",
    "        indices = [ref.index(tok) for tok in hyp if tok in ref]\n",
    "        chunks = 1\n",
    "        for i in range(1, len(indices)):\n",
    "            if indices[i] != indices[i-1] + 1:\n",
    "                chunks += 1\n",
    "        return chunks\n",
    "\n",
    "    ch = count_chunks(reference_tokens, hypothesis_tokens)\n",
    "\n",
    "    # Step 5: Penalty\n",
    "    penalty = gamma * (ch / m) ** beta\n",
    "\n",
    "    # Step 6: Final METEOR\n",
    "    meteor_score = f_mean * (1 - penalty)\n",
    "    return round(meteor_score, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "858b54ee-7923-4151-ac68-c73e786b1eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METEOR Score: 0.333\n"
     ]
    }
   ],
   "source": [
    "reference = ['‡§∞‡§æ‡§Æ‡§É', '‡§µ‡§®‡§Ç', '‡§ó‡§ö‡•ç‡§õ‡§§‡§ø']\n",
    "hypothesis = ['‡§ó‡§ö‡•ç‡§õ‡§§‡§ø', '‡§∞‡§æ‡§Æ‡§É', '‡§µ‡§®‡§Ç']\n",
    "\n",
    "score = compute_meteor(reference, hypothesis)\n",
    "print(f\"METEOR Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66696f37-e192-4a74-958d-80b89e52d591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "596d51e1-b83b-4bc6-93c2-91bd886b3349",
   "metadata": {},
   "source": [
    "## ROUGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076b4df2-40c6-44b9-aa66-a2df73cc5ad5",
   "metadata": {},
   "source": [
    "### ROUGE-L "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9fa1fc9-62bc-4ea2-a7ad-ae9cc269dcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8346b8e-b0e4-4023-9922-062cd55e66d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcs(X, Y):\n",
    "    m, n = len(X), len(Y)\n",
    "    dp = [[0]*(n+1) for _ in range(m+1)]\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if X[i] == Y[j]:\n",
    "                dp[i+1][j+1] = dp[i][j] + 1\n",
    "            else:\n",
    "                dp[i+1][j+1] = max(dp[i][j+1], dp[i+1][j])\n",
    "    return dp[m][n]\n",
    "\n",
    "def compute_rouge_n(reference, hypothesis, n):\n",
    "    ref_ngrams = Counter([tuple(reference[i:i+n]) for i in range(len(reference)-n+1)])\n",
    "    hyp_ngrams = Counter([tuple(hypothesis[i:i+n]) for i in range(len(hypothesis)-n+1)])\n",
    "    match = sum((ref_ngrams & hyp_ngrams).values())\n",
    "    total = sum(ref_ngrams.values())\n",
    "    return round(match / total, 3) if total > 0 else 0.0\n",
    "\n",
    "def compute_rouge_l(reference, hypothesis, beta=1.2):\n",
    "    lcs_len = lcs(reference, hypothesis)\n",
    "    m, n = len(reference), len(hypothesis)\n",
    "    recall = lcs_len / m if m else 0.0\n",
    "    precision = lcs_len / n if n else 0.0\n",
    "    if recall == 0 or precision == 0:\n",
    "        return 0.0\n",
    "    f1 = (1 + beta**2) * recall * precision / (recall + beta**2 * precision)\n",
    "    return round(f1, 3)\n",
    "\n",
    "def compute_rouge_w(reference, hypothesis, beta=1.2):\n",
    "    # Weighted LCS: count squares of consecutive matches\n",
    "    score = 0\n",
    "    i = j = 0\n",
    "    ref_len = len(reference)\n",
    "    hyp_len = len(hypothesis)\n",
    "    matched_lengths = []\n",
    "    while i < ref_len:\n",
    "        length = 0\n",
    "        while j < hyp_len and reference[i] != hypothesis[j]:\n",
    "            j += 1\n",
    "        while i < ref_len and j < hyp_len and reference[i] == hypothesis[j]:\n",
    "            length += 1\n",
    "            i += 1\n",
    "            j += 1\n",
    "        if length > 0:\n",
    "            matched_lengths.append(length)\n",
    "        else:\n",
    "            i += 1\n",
    "    wlcs = sum(l**2 for l in matched_lengths)\n",
    "    f = lambda x: x**2\n",
    "    recall = wlcs / f(len(reference)) if reference else 0.0\n",
    "    precision = wlcs / f(len(hypothesis)) if hypothesis else 0.0\n",
    "    if recall == 0 or precision == 0:\n",
    "        return 0.0\n",
    "    f1 = (1 + beta**2) * recall * precision / (recall + beta**2 * precision)\n",
    "    return round(f1, 3)\n",
    "\n",
    "def skip_bigrams(tokens):\n",
    "    return set(combinations(tokens, 2))\n",
    "\n",
    "def compute_rouge_s(reference, hypothesis, beta=1.0):\n",
    "    ref_sb = skip_bigrams(reference)\n",
    "    hyp_sb = skip_bigrams(hypothesis)\n",
    "    match = len(ref_sb & hyp_sb)\n",
    "    recall = match / len(ref_sb) if ref_sb else 0.0\n",
    "    precision = match / len(hyp_sb) if hyp_sb else 0.0\n",
    "    if recall == 0 or precision == 0:\n",
    "        return 0.0\n",
    "    f1 = (1 + beta**2) * precision * recall / (recall + beta**2 * precision)\n",
    "    return round(f1, 3)\n",
    "\n",
    "def compute_rouge_su(reference, hypothesis, beta=1.0):\n",
    "    ref_sb = skip_bigrams(reference)\n",
    "    hyp_sb = skip_bigrams(hypothesis)\n",
    "    ref_uni = set(reference)\n",
    "    hyp_uni = set(hypothesis)\n",
    "    sb_match = len(ref_sb & hyp_sb)\n",
    "    uni_match = len(ref_uni & hyp_uni)\n",
    "    p = (sb_match + uni_match) / (len(hyp_sb) + len(hyp_uni)) if (len(hyp_sb) + len(hyp_uni)) > 0 else 0.0\n",
    "    r = (sb_match + uni_match) / (len(ref_sb) + len(ref_uni)) if (len(ref_sb) + len(ref_uni)) > 0 else 0.0\n",
    "    if p == 0 or r == 0:\n",
    "        return 0.0\n",
    "    f1 = (1 + beta**2) * p * r / (r + beta**2 * p)\n",
    "    return round(f1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62771b0c-ddd6-41a0-85ee-3978b34a32ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = ['‡§∞‡§æ‡§Æ‡§É', '‡§µ‡§®‡§Ç', '‡§ó‡§ö‡•ç‡§õ‡§§‡§ø']\n",
    "hypothesis = ['‡§ó‡§ö‡•ç‡§õ‡§§‡§ø', '‡§∞‡§æ‡§Æ‡§É', '‡§µ‡§®‡§Ç']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df6dda1a-81bc-4920-a247-501ce525cf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams 1.0\n",
      "Bigrams 0.5\n"
     ]
    }
   ],
   "source": [
    "# Example: ROUGE-N\n",
    "print(\"Unigrams\",compute_rouge_n(reference, hypothesis, n=1))\n",
    "print(\"Bigrams\",compute_rouge_n(reference, hypothesis, n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b47def39-b110-451d-8a23-8bc5e40f37cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.667"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: ROUGE-L\n",
    "compute_rouge_l(reference, hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee042dd5-7843-4680-b82c-3815e3243470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.444"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: ROUGE-W\n",
    "compute_rouge_w(reference, hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af0abc69-04e7-4bb7-afe9-78f6b94004b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.333"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: ROUGE-S\n",
    "compute_rouge_s(reference, hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e85af13-1848-476d-9239-42fa7ffcc757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.667"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: ROUGE-SU\n",
    "compute_rouge_su(reference, hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e03b4c-2a1a-4bc3-b987-21c4aac89ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e8cba3e-587f-4ced-b7b2-d990e475205d",
   "metadata": {},
   "source": [
    "## ChrF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31934fbf-6634-41d2-9c8f-d8636a706988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "646cb479-df69-4510-a318-aba058227938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_ngrams(text, n):\n",
    "    return [text[i:i+n] for i in range(len(text) - n + 1)]\n",
    "\n",
    "def compute_chrf(reference, hypothesis, n=3, beta=2):\n",
    "    ref_text = ''.join(reference)\n",
    "    hyp_text = ''.join(hypothesis)\n",
    "\n",
    "    ref_ngrams = get_char_ngrams(ref_text, n)\n",
    "    hyp_ngrams = get_char_ngrams(hyp_text, n)\n",
    "\n",
    "    match = len(set(ref_ngrams) & set(hyp_ngrams))\n",
    "    precision = match / len(hyp_ngrams) if hyp_ngrams else 0.0\n",
    "    recall = match / len(ref_ngrams) if ref_ngrams else 0.0\n",
    "\n",
    "    if precision == 0 or recall == 0:\n",
    "        return 0.0\n",
    "\n",
    "    chrf = (1 + beta**2) * precision * recall / (beta**2 * precision + recall)\n",
    "    return round(chrf, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4fa12788-8a2b-4e07-94d9-98f144c4a788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChrF Score: 0.526\n"
     ]
    }
   ],
   "source": [
    "reference = ['‡§∞‡§æ‡§Æ‡§É', '‡§µ‡§®‡§Ç', '‡§ó‡§ö‡•ç‡§õ‡§§‡§ø‡•§']\n",
    "hypothesis = ['‡§∞‡§æ‡§Æ‡§É', '‡§µ‡§®‡§Ç', '‡§ó‡§§‡§É‡•§']\n",
    "\n",
    "score = compute_chrf(reference, hypothesis, n=3, beta=2)\n",
    "print(f\"ChrF Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869dc1d9-063e-4baa-9bf6-532a20cc8ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f89a9fc8-189f-49a1-bb7e-ceb0ba4620fb",
   "metadata": {},
   "source": [
    "## ChrF++ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f2d87e0-99da-4fc5-9e6e-98e7c172dd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c7cc4bd-0432-43f0-b7f6-2579b2dd2ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_ngrams(text, n):\n",
    "    return [text[i:i+n] for i in range(len(text) - n + 1)]\n",
    "\n",
    "def get_word_ngrams(tokens, n):\n",
    "    return [' '.join(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "\n",
    "def compute_chrf_plus(reference_tokens, hypothesis_tokens, n_char=6, n_word=2, beta=2):\n",
    "    ref_char = ''.join(reference_tokens)\n",
    "    hyp_char = ''.join(hypothesis_tokens)\n",
    "\n",
    "    # Character n-grams\n",
    "    ref_char_ngrams = Counter(get_char_ngrams(ref_char, n_char))\n",
    "    hyp_char_ngrams = Counter(get_char_ngrams(hyp_char, n_char))\n",
    "    char_match = sum((ref_char_ngrams & hyp_char_ngrams).values())\n",
    "\n",
    "    prec_char = char_match / sum(hyp_char_ngrams.values()) if hyp_char_ngrams else 0\n",
    "    rec_char = char_match / sum(ref_char_ngrams.values()) if ref_char_ngrams else 0\n",
    "\n",
    "    # Word n-grams\n",
    "    ref_word_ngrams = Counter(get_word_ngrams(reference_tokens, n_word))\n",
    "    hyp_word_ngrams = Counter(get_word_ngrams(hypothesis_tokens, n_word))\n",
    "    word_match = sum((ref_word_ngrams & hyp_word_ngrams).values())\n",
    "\n",
    "    prec_word = word_match / sum(hyp_word_ngrams.values()) if hyp_word_ngrams else 0\n",
    "    rec_word = word_match / sum(ref_word_ngrams.values()) if ref_word_ngrams else 0\n",
    "\n",
    "    # Combined precision and recall\n",
    "    precision = (prec_char + prec_word) / 2\n",
    "    recall = (rec_char + rec_word) / 2\n",
    "\n",
    "    if precision == 0 or recall == 0:\n",
    "        return 0.0\n",
    "\n",
    "    chrfpp = (1 + beta**2) * precision * recall / (beta**2 * precision + recall)\n",
    "    return round(chrfpp, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20d9868e-a57d-45b1-b794-a775b980773c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChrF++ Score: 0.431\n"
     ]
    }
   ],
   "source": [
    "reference = ['‡§∞‡§æ‡§Æ‡§É', '‡§µ‡§®‡§Ç', '‡§ó‡§ö‡•ç‡§õ‡§§‡§ø‡•§']\n",
    "hypothesis = ['‡§∞‡§æ‡§Æ‡§É', '‡§µ‡§®‡§Ç', '‡§ó‡§§‡§É‡•§']\n",
    "\n",
    "score = compute_chrf_plus(reference, hypothesis)\n",
    "print(f\"ChrF++ Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134a5e6b-6f72-465d-920b-0b28b588cdf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bec4fcc-ef09-4a31-83ea-8f94a0babda9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56bd81a0-e270-4ac8-ad23-f3ea3224b359",
   "metadata": {},
   "source": [
    "## Translation Edit Rate (TER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "276b8c49-1ada-49b8-b0ec-4486af5b02ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ter(reference_tokens, hypothesis_tokens):\n",
    "    \"\"\"\n",
    "    Simplified TER calculation: only counts insertions, deletions, substitutions,\n",
    "    and a basic reordering shift as 1 edit each. More advanced TER needs alignment tools.\n",
    "\n",
    "    Args:\n",
    "        reference_tokens (list): Tokenized reference sentence\n",
    "        hypothesis_tokens (list): Tokenized hypothesis sentence\n",
    "\n",
    "    Returns:\n",
    "        float: TER score\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from difflib import SequenceMatcher\n",
    "\n",
    "    ref_len = len(reference_tokens)\n",
    "    if ref_len == 0:\n",
    "        return 1.0 if len(hypothesis_tokens) > 0 else 0.0\n",
    "\n",
    "    matcher = SequenceMatcher(None, reference_tokens, hypothesis_tokens)\n",
    "    edits = 0\n",
    "\n",
    "    for opcode in matcher.get_opcodes():\n",
    "        tag, i1, i2, j1, j2 = opcode\n",
    "        if tag != 'equal':\n",
    "            edits += max(i2 - i1, j2 - j1)\n",
    "\n",
    "    ter_score = edits / ref_len\n",
    "    return round(ter_score, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b8b60426-9f5d-4cb6-acba-e36fee495a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TER (Perfect Match): 0.0\n",
      "TER (Shift): 0.667\n",
      "TER (Missing Word): 0.333\n"
     ]
    }
   ],
   "source": [
    "ref = ['‡§∞‡§æ‡§Æ‡§É', '‡§µ‡§®‡§Ç', '‡§ó‡§ö‡•ç‡§õ‡§§‡§ø']\n",
    "hyp1 = ['‡§∞‡§æ‡§Æ‡§É', '‡§µ‡§®‡§Ç', '‡§ó‡§ö‡•ç‡§õ‡§§‡§ø']  # TER = 0\n",
    "hyp2 = ['‡§µ‡§®‡§Ç', '‡§∞‡§æ‡§Æ‡§É', '‡§ó‡§ö‡•ç‡§õ‡§§‡§ø']  # TER = 1/3 ‚âà 0.33\n",
    "hyp3 = ['‡§∞‡§æ‡§Æ‡§É', '‡§ó‡§ö‡•ç‡§õ‡§§‡§ø']         # TER = 1/3 ‚âà 0.33\n",
    "\n",
    "print(\"TER (Perfect Match):\", compute_ter(ref, hyp1))\n",
    "print(\"TER (Shift):\", compute_ter(ref, hyp2))\n",
    "print(\"TER (Missing Word):\", compute_ter(ref, hyp3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989922c4-ceeb-4047-adab-e64e64e2f4c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72eea82-978c-4551-9ee5-9924074c3f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdfebc15-f3b4-4b5a-87a9-d65085d42f99",
   "metadata": {},
   "source": [
    "## TER-M (Translation Edit Rate with Morphology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be03cc1f-b1f7-4438-bafc-d9cd5980c0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d2e4dc7-e3dd-4739-bba7-d8eb5e6551d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration, we simulate a morphological normalization function.\n",
    "def simple_lemmatizer(tokens):\n",
    "    \"\"\"\n",
    "    Simulated lemmatizer for Sanskrit-like text.\n",
    "    Replace this with an actual lemmatizer for production.\n",
    "    \"\"\"\n",
    "    lemma_map = {\n",
    "        '‡§∞‡§æ‡§Æ‡§É': '‡§∞‡§æ‡§Æ',\n",
    "        '‡§∞‡§æ‡§Æ‡§Ç': '‡§∞‡§æ‡§Æ',\n",
    "        '‡§ó‡§ö‡•ç‡§õ‡§§‡§ø': '‡§ó‡§Æ‡•ç',\n",
    "        '‡§ó‡§§‡§É': '‡§ó‡§Æ‡•ç',\n",
    "        '‡§µ‡§®‡§Ç': '‡§µ‡§®',\n",
    "        '‡§ó‡§ö‡•ç‡§õ‡§æ‡§Æ‡§ø': '‡§ó‡§Æ‡•ç'\n",
    "    }\n",
    "    return [lemma_map.get(token, token) for token in tokens]\n",
    "\n",
    "def compute_ter_m(reference_tokens, hypothesis_tokens):\n",
    "    \"\"\"\n",
    "    Computes TER-M (Translation Edit Rate with Morphology Awareness)\n",
    "    by lemmatizing both reference and hypothesis before computing TER.\n",
    "    \"\"\"\n",
    "    norm_ref = simple_lemmatizer(reference_tokens)\n",
    "    norm_hyp = simple_lemmatizer(hypothesis_tokens)\n",
    "\n",
    "    ref_len = len(norm_ref)\n",
    "    if ref_len == 0:\n",
    "        return 1.0 if len(norm_hyp) > 0 else 0.0\n",
    "\n",
    "    matcher = SequenceMatcher(None, norm_ref, norm_hyp)\n",
    "    edits = 0\n",
    "    for opcode in matcher.get_opcodes():\n",
    "        tag, i1, i2, j1, j2 = opcode\n",
    "        if tag != 'equal':\n",
    "            edits += max(i2 - i1, j2 - j1)\n",
    "\n",
    "    return round(edits / ref_len, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c9880bff-1a96-4c91-b657-925a5c534484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TER-M (Order Change): 0.667\n",
      "TER-M (Lemma Match): 0.667\n"
     ]
    }
   ],
   "source": [
    "ref = ['‡§∞‡§æ‡§Æ‡§É', '‡§µ‡§®‡§Ç', '‡§ó‡§ö‡•ç‡§õ‡§§‡§ø']\n",
    "hyp1 = ['‡§µ‡§®‡§Ç', '‡§ó‡§ö‡•ç‡§õ‡§§‡§ø', '‡§∞‡§æ‡§Æ‡§É']  # Different order but lemmatized match\n",
    "hyp2 = ['‡§µ‡§®‡§Ç', '‡§ó‡§ö‡•ç‡§õ‡§§‡§ø', '‡§∞‡§æ‡§Æ‡§Ç']   # Slightly different surface form, same lemma\n",
    "\n",
    "print(\"TER-M (Order Change):\", compute_ter_m(ref, hyp1))  # Expected: 0.0\n",
    "print(\"TER-M (Lemma Match):\", compute_ter_m(ref, hyp2))  # Expected: 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e616e9-e35c-44f2-9da9-3561211fcc67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54528897-a695-4f77-98c6-eb14c9bbbd41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd6213f5-860b-4a34-a3ec-4d9da18d4924",
   "metadata": {},
   "source": [
    "## Exact Match (EM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a609d11f-3f30-4e73-a7cb-ca49764dabb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_exact_match(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Computes Exact Match (EM) score for a single example.\n",
    "\n",
    "    Args:\n",
    "        reference (list or str): Reference tokens or sentence string\n",
    "        hypothesis (list or str): Predicted tokens or sentence string\n",
    "\n",
    "    Returns:\n",
    "        int: 1 if exact match, else 0\n",
    "    \"\"\"\n",
    "    if isinstance(reference, list):\n",
    "        reference = ' '.join(reference)\n",
    "    if isinstance(hypothesis, list):\n",
    "        hypothesis = ' '.join(hypothesis)\n",
    "\n",
    "    return int(reference.strip() == hypothesis.strip())\n",
    "\n",
    "def compute_exact_match_score(references, hypotheses):\n",
    "    \"\"\"\n",
    "    Computes Exact Match score for a dataset of predictions.\n",
    "\n",
    "    Args:\n",
    "        references (list): List of reference strings or token lists\n",
    "        hypotheses (list): List of predicted strings or token lists\n",
    "\n",
    "    Returns:\n",
    "        float: EM score as a proportion (0 to 1)\n",
    "    \"\"\"\n",
    "    total = len(references)\n",
    "    matches = sum(compute_exact_match(r, h) for r, h in zip(references, hypotheses))\n",
    "    return round(matches / total, 3) if total > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9a073249-087b-4e2d-bc4d-ef07408916af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match Score: 0.333\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "ref = ['‡§∞‡§æ‡§Æ‡§É ‡§µ‡§®‡§Ç ‡§ó‡§ö‡•ç‡§õ‡§§‡§ø']\n",
    "hyp1 = ['‡§∞‡§æ‡§Æ‡§É ‡§µ‡§®‡§Ç ‡§ó‡§ö‡•ç‡§õ‡§§‡§ø']     # Exact match\n",
    "hyp2 = ['‡§ó‡§ö‡•ç‡§õ‡§§‡§ø ‡§∞‡§æ‡§Æ‡§É ‡§µ‡§®‡§Ç']     # Order mismatch\n",
    "hyp3 = ['‡§∞‡§æ‡§Æ‡§É ‡§ó‡§ö‡•ç‡§õ‡§§‡§ø']         # Missing word\n",
    "\n",
    "refs = ref * 3\n",
    "hyps = [hyp1[0], hyp2[0], hyp3[0]]\n",
    "print(\"Exact Match Score:\", compute_exact_match_score(refs, hyps))  # Expected: 0.333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc09ec4-9330-4ecb-8211-b45af8316298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0513c2-e958-4de2-b6ba-df91f0410d13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36c187a7-2c96-4416-93dd-4cf44552977f",
   "metadata": {},
   "source": [
    "# Embedding-based Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3180a43c-7a2b-4bdf-bc94-9bcb94684c68",
   "metadata": {},
   "source": [
    "## BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6d962317-24f9-4f93-b639-045f4692b653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "521fb7d1-a3df-4a9c-8428-9d9d7d20b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bertscore(reference_list, hypothesis_list, lang='en', model_type='xlm-roberta-base', verbose=False):\n",
    "    \"\"\"\n",
    "    Compute BERTScore (Precision, Recall, F1) using contextual embeddings.\n",
    "\n",
    "    Args:\n",
    "        reference_list (list): List of reference strings\n",
    "        hypothesis_list (list): List of hypothesis strings\n",
    "        lang (str): Language code (e.g., 'en', 'hi', 'sa')\n",
    "        model_type (str): HuggingFace model type\n",
    "        verbose (bool): If True, prints each sentence's score\n",
    "\n",
    "    Returns:\n",
    "        tuple: Average Precision, Recall, and F1 score\n",
    "    \"\"\"\n",
    "    P, R, F1 = score(hypothesis_list, reference_list, lang=lang, model_type=model_type, verbose=verbose)\n",
    "    return round(P.mean().item(), 4), round(R.mean().item(), 4), round(F1.mean().item(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8658ff53-5fa5-4262-9656-f65a6f65200f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ce3ef181694579831fad65701f8fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore-P: 0.964\n",
      "BERTScore-R: 0.9264\n",
      "BERTScore-F1: 0.9448\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "ref = ['‡§∞‡§æ‡§Æ‡§É ‡§µ‡§®‡§Ç ‡§ó‡§ö‡•ç‡§õ‡§§‡§ø‡•§']\n",
    "hyp = ['‡§∞‡§æ‡§Æ‡§É ‡§µ‡§®‡§Ç ‡§ó‡§§‡§É‡•§']\n",
    "\n",
    "p, r, f1 = compute_bertscore(ref, hyp, lang='sa', model_type='xlm-roberta-base')\n",
    "print(f\"BERTScore-P: {p}\")\n",
    "print(f\"BERTScore-R: {r}\")\n",
    "print(f\"BERTScore-F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30d4859-6301-46ab-8aa6-f55fa0069a63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85ae930c-610a-472f-8c78-03860e7d6df0",
   "metadata": {},
   "source": [
    "## COMET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "886ca42a-c877-4ac2-8cdc-3e2b4fd8e8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet import download_model, load_from_checkpoint\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "435f5417-3744-40a1-8f9a-df7b93c52dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d462c96bac44b2aa8826a3bdbb7c82c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\Joshuva\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da\\snapshots\\2760a223ac957f30acfb18c8aa649b01cf1d75f2\\checkpoints\\model.ckpt`\n",
      "Encoder model frozen.\n",
      "C:\\Users\\Joshuva\\AppData\\Roaming\\Python\\Python312\\site-packages\\pytorch_lightning\\core\\saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMET Score: [0.8752]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# COMET Scorer\n",
    "class COMETScorer:\n",
    "    def __init__(self, model_name=\"Unbabel/wmt22-comet-da\"):\n",
    "        self.model_path = download_model(model_name)\n",
    "        self.model = load_from_checkpoint(self.model_path)\n",
    "\n",
    "    def score(self, sources, hypotheses, references):\n",
    "        data = [{\"src\": s, \"mt\": h, \"ref\": r} for s, h, r in zip(sources, hypotheses, references)]\n",
    "        scores = self.model.predict(data, batch_size=1, gpus=1 if torch.cuda.is_available() else 0)[\"scores\"]\n",
    "        return [round(s, 4) for s in scores]\n",
    "\n",
    "# ==== Example Data ====\n",
    "srcs = [\"‡§∞‡§æ‡§Æ ‡§ú‡§Ç‡§ó‡§≤ ‡§Æ‡•á‡§Ç ‡§ó‡§Ø‡§æ‡•§\"]                    # Source in Hindi\n",
    "hyps = [\"‡§∞‡§æ‡§Æ‡§É ‡§µ‡§®‡§Ç ‡§ó‡§§‡§É‡•§\"]                        # Hypothesis in Sanskrit (system output)\n",
    "refs = [\"‡§∞‡§æ‡§Æ‡§É ‡§µ‡§®‡§Ç ‡§ó‡§ö‡•ç‡§õ‡§§‡§ø‡•§\"]                     # Reference in Sanskrit (human translation)\n",
    "\n",
    "# ==== Compute COMET Score ====\n",
    "comet = COMETScorer()\n",
    "print(\"COMET Score:\", comet.score(srcs, hyps, refs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc9bc4-0cc1-46e4-b011-4569d9910845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b66964a-2b65-408d-b1a8-78d73d88367d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c857e6-ca5a-4be6-a337-dc8fd32065c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cd00171-ad07-4944-8986-f422a12ac77e",
   "metadata": {},
   "source": [
    "## LaBSE (Language-agnostic BERT Sentence Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "86bf7c2d-863f-4153-87cf-d0e51912957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1782d76b-d036-4275-89bf-304887e29061",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaBSEScorer:\n",
    "    def __init__(self, model_name='sentence-transformers/LaBSE'):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "\n",
    "    def embed(self, sentence):\n",
    "        with torch.no_grad():\n",
    "            inputs = self.tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "            output = self.model(**inputs)\n",
    "            cls_embedding = output.last_hidden_state[:, 0, :]\n",
    "            return F.normalize(cls_embedding, p=2, dim=1)\n",
    "\n",
    "    def score(self, ref_sentence, hyp_sentence):\n",
    "        ref_emb = self.embed(ref_sentence)\n",
    "        hyp_emb = self.embed(hyp_sentence)\n",
    "        cosine_sim = F.cosine_similarity(ref_emb, hyp_emb).item()\n",
    "        return round(cosine_sim, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ffded8d0-2f84-4f5d-b3e1-e18ae4882feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a229ddeb224fd5a1e911880d66a622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   6%|5         | 105M/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaBSE Cosine Similarity: 0.96\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "scorer = LaBSEScorer()\n",
    "reference = \"‡§∞‡§æ‡§Æ‡§É ‡§µ‡§®‡§Ç ‡§ó‡§ö‡•ç‡§õ‡§§‡§ø‡•§\"\n",
    "hypothesis = \"‡§∞‡§æ‡§Æ‡§É ‡§µ‡§®‡§Ç ‡§ó‡§§‡§É‡•§\"\n",
    "score = scorer.score(reference, hypothesis)\n",
    "print(f\"LaBSE Cosine Similarity: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb66d60-29c1-42fa-975a-e294f7196cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5cff19-4e30-4c82-93de-986a8594add7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e3c60-725f-4b60-a033-0df594caba6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c75a5cf-f2a7-4555-8c23-2063712cf2bc",
   "metadata": {},
   "source": [
    "## YiSiScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d0dc79b-81aa-417e-89c1-5f5fff8fc693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f8cbb01-37c2-4f76-8bea-d22aae58bdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YiSi-0: 0.333\n",
      "YiSi-1: 0.579\n",
      "YiSi-2: 0.915\n",
      "YiSi-3: 0.7757\n"
     ]
    }
   ],
   "source": [
    "class YiSiScorer:\n",
    "    def __init__(self, contextual_model='sentence-transformers/LaBSE'):\n",
    "        # Load contextual model (LaBSE)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(contextual_model)\n",
    "        self.model = AutoModel.from_pretrained(contextual_model)\n",
    "        self.model.eval()\n",
    "\n",
    "        # Load FastText Sanskrit model\n",
    "        fasttext.util.download_model('sa', if_exists='ignore')\n",
    "        self.static_model = fasttext.load_model('cc.sa.300.bin')\n",
    "\n",
    "    def cosine_similarity(self, vec1, vec2):\n",
    "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2) + 1e-8)\n",
    "\n",
    "    def compute_yisi_0(self, ref_tokens, hyp_tokens):\n",
    "        ref_set = set(ref_tokens)\n",
    "        hyp_set = set(hyp_tokens)\n",
    "        match = len(ref_set & hyp_set)\n",
    "        P = match / len(hyp_set) if hyp_set else 0.0\n",
    "        R = match / len(ref_set) if ref_set else 0.0\n",
    "        return round((2 * P * R) / (P + R), 3) if (P + R) else 0.0\n",
    "\n",
    "    def compute_yisi_1(self, ref_tokens, hyp_tokens):\n",
    "        def max_sim(word, other_words):\n",
    "            vec1 = self.static_model.get_word_vector(word)\n",
    "            return max(\n",
    "                self.cosine_similarity(vec1, self.static_model.get_word_vector(w))\n",
    "                for w in other_words\n",
    "            )\n",
    "\n",
    "        P = sum(max_sim(w, ref_tokens) for w in hyp_tokens) / len(hyp_tokens)\n",
    "        R = sum(max_sim(w, hyp_tokens) for w in ref_tokens) / len(ref_tokens)\n",
    "        return round((2 * P * R) / (P + R), 3) if (P + R) else 0.0\n",
    "\n",
    "    def compute_yisi_2(self, ref_tokens, hyp_tokens):\n",
    "        with torch.no_grad():\n",
    "            inputs_ref = self.tokenizer(ref_tokens, return_tensors='pt', is_split_into_words=True, padding=True, truncation=True)\n",
    "            inputs_hyp = self.tokenizer(hyp_tokens, return_tensors='pt', is_split_into_words=True, padding=True, truncation=True)\n",
    "            ref_embs = self.model(**inputs_ref).last_hidden_state[0][1:1+len(ref_tokens)]\n",
    "            hyp_embs = self.model(**inputs_hyp).last_hidden_state[0][1:1+len(hyp_tokens)]\n",
    "\n",
    "            P = torch.stack([F.cosine_similarity(h.unsqueeze(0), ref_embs).max() for h in hyp_embs]).mean().item()\n",
    "            R = torch.stack([F.cosine_similarity(r.unsqueeze(0), hyp_embs).max() for r in ref_embs]).mean().item()\n",
    "            return round((2 * P * R) / (P + R), 3) if (P + R) else 0.0\n",
    "\n",
    "    def compute_yisi_3(self, ref_sentence, hyp_sentence):\n",
    "        with torch.no_grad():\n",
    "            inputs_ref = self.tokenizer(ref_sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "            inputs_hyp = self.tokenizer(hyp_sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "            ref_emb = self.model(**inputs_ref).last_hidden_state[:, 0, :]\n",
    "            hyp_emb = self.model(**inputs_hyp).last_hidden_state[:, 0, :]\n",
    "            sim = F.cosine_similarity(ref_emb, hyp_emb).item()\n",
    "            return round(sim, 4)\n",
    "\n",
    "# ========== Test Example ==========\n",
    "scorer = YiSiScorer()\n",
    "\n",
    "# Token level\n",
    "ref_tokens = ['‡§∞‡§æ‡§Æ‡§É', '‡§µ‡§®‡§Ç', '‡§ó‡§ö‡•ç‡§õ‡§§‡§ø']\n",
    "hyp_tokens = ['‡§∞‡§æ‡§Æ‡§É', '‡§Ö‡§∞‡§£‡•ç‡§Ø‡§Ç', '‡§Ø‡§æ‡§§‡§ø']\n",
    "\n",
    "# Sentence level\n",
    "ref_sentence = \"‡§∞‡§æ‡§Æ‡§É ‡§µ‡§®‡§Ç ‡§ó‡§ö‡•ç‡§õ‡§§‡§ø‡•§\"\n",
    "hyp_sentence = \"Rama goes to the forest.\"\n",
    "\n",
    "print(\"YiSi-0:\", scorer.compute_yisi_0(ref_tokens, hyp_tokens))\n",
    "print(\"YiSi-1:\", scorer.compute_yisi_1(ref_tokens, hyp_tokens))\n",
    "print(\"YiSi-2:\", scorer.compute_yisi_2(ref_tokens, hyp_tokens))\n",
    "print(\"YiSi-3:\", scorer.compute_yisi_3(ref_sentence, hyp_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fed1ce9-b62b-4a2d-9988-b12526b7b4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca9d155-10f5-45e4-9974-c3b4c6b3665b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f645c0-573a-4ea4-9e57-87ed1118a5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae37e75-665f-4d04-9bbf-7a4be5c907ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6648e5f-3181-4fd1-b05d-737f171a7979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10778c98-14e0-4032-8133-d3a4558bdc99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c4c99a3-022a-4d72-95e3-fc8985338a58",
   "metadata": {},
   "source": [
    "## X(NLI)-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "01505ab6-1c71-480f-9f39-18cfde7643c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "17de3dec-0727-4581-9d7b-0418bb97dc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XNLIRScorer:\n",
    "    def __init__(self, model_name=\"joeddav/xlm-roberta-large-xnli\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "\n",
    "    def predict_label(self, premise, hypothesis):\n",
    "        inputs = self.tokenizer(premise, hypothesis, return_tensors='pt', truncation=True)\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**inputs).logits\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        label_id = torch.argmax(probs, dim=1).item()\n",
    "        return label_id  # 0: entailment, 1: neutral, 2: contradiction\n",
    "\n",
    "    def compute_xnli_r(self, reference, hypotheses):\n",
    "        entail_count = 0\n",
    "        for hyp in hypotheses:\n",
    "            label = self.predict_label(reference, hyp)\n",
    "            if label == 0:\n",
    "                entail_count += 1\n",
    "        return round(entail_count / len(hypotheses), 3) if hypotheses else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "269911f7-a3ee-49ea-b931-4c54d6a55a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7ab5cf16df4cfea366f12ce04bdc03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X(NLI)-R Score: 0.333\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    scorer = XNLIRScorer()\n",
    "    reference = \"‡§∞‡§æ‡§Æ‡§É ‡§µ‡§®‡§Ç ‡§ó‡§ö‡•ç‡§õ‡§§‡§ø‡•§\"\n",
    "    hypotheses = [\n",
    "        \"‡§∞‡§æ‡§Æ‡§É ‡§µ‡§®‡§Ç ‡§ó‡§§‡§É‡•§\",       # Entailment\n",
    "        \"‡§∞‡§æ‡§Æ‡§É ‡§µ‡§®‡§Ç ‡§® ‡§ó‡§ö‡•ç‡§õ‡§§‡§ø‡•§\",  # Contradiction\n",
    "        \"‡§∞‡§æ‡§Æ‡§É ‡§™‡•Å‡§∑‡•ç‡§™‡§Ç ‡§™‡§∂‡•ç‡§Ø‡§§‡§ø‡•§\"   # Neutral\n",
    "    ]\n",
    "    score = scorer.compute_xnli_r(reference, hypotheses)\n",
    "    print(f\"X(NLI)-R Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f45e53-5c94-4780-9d0d-843d211af28a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f0de2c-c188-4f90-89e1-a006212642eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cc7ca15-ede7-4891-beb2-745f429a0091",
   "metadata": {},
   "source": [
    "## X(NLI)-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4ecf53c8-a1dd-4778-864e-4ac1967ccbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "13e4ace8-f3ec-4d5b-a810-0ca738b8e5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothesis: ‡§∞‡§æ‡§Æ‡§É ‡§µ‡§®‡§Ç ‡§ó‡§§‡§É‡•§\n",
      "‚Üí Prediction: Entailment\n",
      "\n",
      "Hypothesis: ‡§∞‡§æ‡§Æ‡§É ‡§™‡•Å‡§∑‡•ç‡§™‡§Ç ‡§™‡§∂‡•ç‡§Ø‡§§‡§ø‡•§\n",
      "‚Üí Prediction: Neutral\n",
      "\n",
      "Hypothesis: ‡§∞‡§æ‡§Æ‡§É ‡§µ‡§®‡§Ç ‡§® ‡§ó‡§ö‡•ç‡§õ‡§§‡§ø‡•§\n",
      "‚Üí Prediction: Contradiction\n",
      "\n",
      "‚úÖ X(NLI)-D Score: 0.667\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained XNLI model (XLM-RoBERTa)\n",
    "model_name = \"joeddav/xlm-roberta-large-xnli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Input: Reference (premise) and hypotheses\n",
    "reference = \"‡§∞‡§æ‡§Æ‡§É ‡§µ‡§®‡§Ç ‡§ó‡§ö‡•ç‡§õ‡§§‡§ø‡•§\"\n",
    "hypotheses = [\n",
    "    \"‡§∞‡§æ‡§Æ‡§É ‡§µ‡§®‡§Ç ‡§ó‡§§‡§É‡•§\",        # Entailment\n",
    "    \"‡§∞‡§æ‡§Æ‡§É ‡§™‡•Å‡§∑‡•ç‡§™‡§Ç ‡§™‡§∂‡•ç‡§Ø‡§§‡§ø‡•§\",   # Neutral\n",
    "    \"‡§∞‡§æ‡§Æ‡§É ‡§µ‡§®‡§Ç ‡§® ‡§ó‡§ö‡•ç‡§õ‡§§‡§ø‡•§\"     # Contradiction\n",
    "]\n",
    "\n",
    "# Class mapping\n",
    "label_map = {0: \"Contradiction\", 1: \"Neutral\", 2: \"Entailment\"}\n",
    "\n",
    "# Count non-contradiction cases\n",
    "non_contradictions = 0\n",
    "\n",
    "for hypo in hypotheses:\n",
    "    inputs = tokenizer(reference, hypo, return_tensors=\"pt\", truncation=True)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    prediction = torch.argmax(F.softmax(logits, dim=1)).item()\n",
    "    result = label_map[prediction]\n",
    "    print(f\"Hypothesis: {hypo}\\n‚Üí Prediction: {result}\\n\")\n",
    "    \n",
    "    if result != \"Contradiction\":\n",
    "        non_contradictions += 1\n",
    "\n",
    "# Compute X(NLI)-D Score\n",
    "xnli_d_score = non_contradictions / len(hypotheses)\n",
    "print(f\"‚úÖ X(NLI)-D Score: {xnli_d_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88d14ec-c5f8-4b52-8eab-bc8227fe6f31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b7b5c3-6820-4fbf-b88e-ee5155f82dab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d57b6a6-af1d-4457-8c05-b8fd74306393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac67c31-bd6f-4914-84a2-2d2b400e9306",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85829353-56a3-459d-98c2-6f160cf5185a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7080ccb-8a1d-45f2-876a-03e86e798762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae10e6-7cd0-49f6-ba28-473708285526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82407b80-2682-418d-8d00-27ada47a0550",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Structural Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a941e0-ea58-447e-9e82-2b1c211b5608",
   "metadata": {},
   "source": [
    "## UAS (Unlabeled Attachment Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459fe57d-a45e-4fa4-bdc9-9bbf2fa9b5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_uas(reference_heads, predicted_heads):\n",
    "    \"\"\"\n",
    "    Computes Unlabeled Attachment Score (UAS)\n",
    "\n",
    "    Args:\n",
    "        reference_heads (list): List of gold head indices or tokens\n",
    "        predicted_heads (list): List of predicted head indices or tokens\n",
    "\n",
    "    Returns:\n",
    "        float: UAS score rounded to 3 decimals\n",
    "    \"\"\"\n",
    "    if len(reference_heads) != len(predicted_heads):\n",
    "        raise ValueError(\"Mismatch in reference and predicted lengths\")\n",
    "\n",
    "    correct = sum(1 for r, p in zip(reference_heads, predicted_heads) if r == p)\n",
    "    total = len(reference_heads)\n",
    "    return round(correct / total, 3) if total > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2745eaf2-f31a-4756-8cc3-c53425adad4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAS Score: 0.667\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "# Tokens: ['‡§∞‡§æ‡§Æ‡§É', '‡§µ‡§®‡§Ç', '‡§ó‡§ö‡•ç‡§õ‡§§‡§ø']\n",
    "# Head values can be actual tokens or their IDs\n",
    "ref_heads = ['‡§ó‡§ö‡•ç‡§õ‡§§‡§ø', '‡§ó‡§ö‡•ç‡§õ‡§§‡§ø', 'ROOT']\n",
    "pred_heads = ['‡§ó‡§ö‡•ç‡§õ‡§§‡§ø', '‡§∞‡§æ‡§Æ‡§É', 'ROOT']\n",
    "\n",
    "uas_score = compute_uas(ref_heads, pred_heads)\n",
    "print(\"UAS Score:\", uas_score)  # Expected: 2/3 = 0.667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fd4c6f-1436-43a0-8fdf-e1cc5c99a763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a714b46-c8f5-4d30-9dd3-7996c83b46d2",
   "metadata": {},
   "source": [
    "## Labeled Attachment Score (LAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "39d89964-99fb-45b1-ae7e-f6c88fd93561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_las(reference_heads, predicted_heads, reference_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Computes Labeled Attachment Score (LAS)\n",
    "\n",
    "    Args:\n",
    "        reference_heads (list): List of gold head indices or tokens\n",
    "        predicted_heads (list): List of predicted head indices or tokens\n",
    "        reference_labels (list): List of gold dependency labels\n",
    "        predicted_labels (list): List of predicted dependency labels\n",
    "\n",
    "    Returns:\n",
    "        float: LAS score rounded to 3 decimals\n",
    "    \"\"\"\n",
    "    if not (len(reference_heads) == len(predicted_heads) == len(reference_labels) == len(predicted_labels)):\n",
    "        raise ValueError(\"All input lists must have the same length\")\n",
    "\n",
    "    correct = sum(\n",
    "        1 for rh, ph, rl, pl in zip(reference_heads, predicted_heads, reference_labels, predicted_labels)\n",
    "        if rh == ph and rl == pl\n",
    "    )\n",
    "    total = len(reference_heads)\n",
    "    return round(correct / total, 3) if total > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6d4a5f9a-9261-422a-bc74-16e51b9879ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAS Score: 0.667\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "ref_heads = ['‡§ó‡§ö‡•ç‡§õ‡§§‡§ø', '‡§ó‡§ö‡•ç‡§õ‡§§‡§ø', 'ROOT']\n",
    "pred_heads = ['‡§ó‡§ö‡•ç‡§õ‡§§‡§ø', '‡§ó‡§ö‡•ç‡§õ‡§§‡§ø', 'ROOT']\n",
    "ref_labels = ['nominal subject', 'obj', 'root']\n",
    "pred_labels = ['nominal subject', 'obl', 'root']\n",
    "\n",
    "las_score = compute_las(ref_heads, pred_heads, ref_labels, pred_labels)\n",
    "print(\"LAS Score:\", las_score)  # Expected: 2/3 = 0.667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451c7863-176a-46da-b5a0-7eb1f7ac75ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7253fd4-8f78-4607-9968-e6e2daa5ea5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2922cf0f-71d3-4899-b070-a62090adee37",
   "metadata": {},
   "source": [
    "## Bits Per Character (BPC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "326147c4-a87b-4f36-9276-ba8fea049e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0daac9ed-d6bb-4fe6-8a37-16bcb0df14ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPC Score: 0.467\n"
     ]
    }
   ],
   "source": [
    "def compute_bpc(probabilities):\n",
    "    \"\"\"\n",
    "    Computes Bits Per Character (BPC) for a given list of predicted probabilities.\n",
    "\n",
    "    Args:\n",
    "        probabilities (list): List of predicted probabilities for correct characters (0 < p <= 1)\n",
    "\n",
    "    Returns:\n",
    "        float: BPC score rounded to 3 decimals\n",
    "    \"\"\"\n",
    "    if not probabilities:\n",
    "        return 0.0\n",
    "\n",
    "    total_log_loss = sum(-math.log2(p) for p in probabilities if p > 0)\n",
    "    return round(total_log_loss / len(probabilities), 3)\n",
    "\n",
    "\n",
    "# Example\n",
    "if __name__ == \"__main__\":\n",
    "    predicted_probs = [\n",
    "        0.8, 0.7, 0.6, 0.9, 0.7,\n",
    "        0.65, 0.7, 0.8, 0.7, 0.6,\n",
    "        0.7, 0.85, 0.6, 0.75, 0.9\n",
    "    ]\n",
    "    bpc_score = compute_bpc(predicted_probs)\n",
    "    print(\"BPC Score:\", bpc_score)  # Expected: ~0.531"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fdc53d-e4b4-400f-9743-c2f64b1b4422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b11369e-c357-46d9-abb6-4add459718fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6203a396-40ca-4d62-ada8-594a0203edf4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Ranking & Retrieval-based Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8391a8-6d66-4db4-a29e-ea4898bf0a06",
   "metadata": {},
   "source": [
    "## Mean Reciprocal Rank (MRR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f15a8e71-9d29-4ea4-b8a7-8c2045bf3824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mrr(rank_list):\n",
    "    \"\"\"\n",
    "    Computes Mean Reciprocal Rank (MRR)\n",
    "\n",
    "    Args:\n",
    "        rank_list (list): A list of integers where each integer is the rank position\n",
    "                          of the first correct answer for each query.\n",
    "\n",
    "    Returns:\n",
    "        float: MRR score rounded to 3 decimals\n",
    "    \"\"\"\n",
    "    if not rank_list:\n",
    "        return 0.0\n",
    "\n",
    "    reciprocal_sum = sum(1.0 / r for r in rank_list if r > 0)\n",
    "    return round(reciprocal_sum / len(rank_list), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e32ae23c-3de9-453e-bdb6-0621e362785a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR Score: 0.611\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "ranks = [1, 2, 3]  # first relevant answer found at positions 1, 2, 3\n",
    "mrr_score = compute_mrr(ranks)\n",
    "print(\"MRR Score:\", mrr_score)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f13314-3e54-4b51-b155-1fa9ac85d71d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c082b317-8028-4da1-9b52-9bd5041c8a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05bbd8ef-4659-4349-9f2c-18b5d49b9b7e",
   "metadata": {},
   "source": [
    "## Mean Average Precision (MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2010ab29-ad8d-401b-a2be-26e5bc9cf457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(relevance_list):\n",
    "    \"\"\"\n",
    "    Computes Average Precision (AP) for a single query\n",
    "\n",
    "    Args:\n",
    "        relevance_list (list): List of 0s and 1s indicating relevance at each rank position\n",
    "\n",
    "    Returns:\n",
    "        float: Average Precision (AP)\n",
    "    \"\"\"\n",
    "    num_relevant = sum(relevance_list)\n",
    "    if num_relevant == 0:\n",
    "        return 0.0\n",
    "\n",
    "    score = 0.0\n",
    "    correct = 0\n",
    "    for i, rel in enumerate(relevance_list):\n",
    "        if rel:\n",
    "            correct += 1\n",
    "            score += correct / (i + 1)\n",
    "    return round(score / num_relevant, 3)\n",
    "\n",
    "def mean_average_precision(all_queries):\n",
    "    \"\"\"\n",
    "    Computes Mean Average Precision (MAP) across multiple queries\n",
    "\n",
    "    Args:\n",
    "        all_queries (list): List of relevance lists, one per query\n",
    "\n",
    "    Returns:\n",
    "        float: Mean Average Precision (MAP)\n",
    "    \"\"\"\n",
    "    if not all_queries:\n",
    "        return 0.0\n",
    "    return round(sum(average_precision(q) for q in all_queries) / len(all_queries), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "24a7c198-5990-4431-89b4-f308b2ea314f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP Score: 0.944\n"
     ]
    }
   ],
   "source": [
    "# rel(k) for 3 queries:\n",
    "queries = [\n",
    "        [1, 0, 1, 0],   # AP = (1 + 2/3)/2 = 0.833\n",
    "        [1, 1, 0],      # AP = (1 + 2/2)/2 = 0.75\n",
    "        [1, 0, 0]       # AP = 1.0\n",
    "    ]\n",
    "\n",
    "map_score = mean_average_precision(queries)\n",
    "print(\"MAP Score:\", map_score)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11f4dfc-3611-4de7-b28a-e8c98de95b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917d84db-563f-41c3-af22-4f7effaf5e52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dea43305-01b3-43f9-91b8-630381b43bcb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Statistical & Human Agreement Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64a353d-1469-4396-94e2-12f587ba1457",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2929d22b-6461-4567-a89f-0a23e47ebd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1641da88-1a42-4012-8ab6-7f2541266ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(probabilities):\n",
    "    \"\"\"\n",
    "    Computes Perplexity for a given list of conditional probabilities.\n",
    "\n",
    "    Args:\n",
    "        probabilities (list): List of P(w_i | w_1^{i-1}) values (0 < p <= 1)\n",
    "\n",
    "    Returns:\n",
    "        float: Perplexity score rounded to 3 decimals\n",
    "    \"\"\"\n",
    "    if not probabilities:\n",
    "        return 0.0\n",
    "\n",
    "    log_sum = sum(math.log(p) for p in probabilities if p > 0)\n",
    "    avg_log = log_sum / len(probabilities)\n",
    "    perplexity = math.exp(-avg_log)\n",
    "    return round(perplexity, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fb8654d4-48b1-4034-ab04-ae29e577a344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 2.924\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "model_probs = [0.4, 0.5, 0.2]  # ‡§∞‡§æ‡§Æ‡§É, ‡§µ‡§®‡§Ç, ‡§ó‡§ö‡•ç‡§õ‡§§‡§ø\n",
    "result = compute_perplexity(model_probs)\n",
    "print(\"Perplexity:\", result)  # Expected: ~2.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2d83df-357e-45aa-a5cf-f1c3d1127dee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3597c576-6686-49c1-abbb-51658af01518",
   "metadata": {},
   "source": [
    "## Token-level F1 Score for Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c3cf319e-7dfb-4fbc-abf0-2007fa7e5663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_token_level_f1(gold_boundaries, predicted_boundaries):\n",
    "    \"\"\"\n",
    "    Compute token-level F1 Score for segmentation.\n",
    "\n",
    "    Args:\n",
    "        gold_boundaries (list): List of boundary indices from gold segmentation.\n",
    "        predicted_boundaries (list): List of boundary indices from system output.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with precision, recall, and F1 score.\n",
    "    \"\"\"\n",
    "    gold_set = set(gold_boundaries)\n",
    "    pred_set = set(predicted_boundaries)\n",
    "\n",
    "    true_positives = len(gold_set & pred_set)\n",
    "    false_positives = len(pred_set - gold_set)\n",
    "    false_negatives = len(gold_set - pred_set)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"precision\": round(precision, 3),\n",
    "        \"recall\": round(recall, 3),\n",
    "        \"f1_score\": round(f1, 3)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "87f89aea-a58c-4df1-9c5e-a6c989719b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-Level Segmentation Evaluation:\n",
      "{'precision': 0.5, 'recall': 0.333, 'f1_score': 0.4}\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "# Reference: ‡§∞‡§æ | ‡§ú‡§æ | ‡§Ö‡§®‡•Å | ‡§ó‡§ö‡•ç‡§õ‡§§‡§ø ‚Üí boundaries at indices 2, 4, 7\n",
    "# Predicted: ‡§∞‡§æ‡§ú‡§æ | ‡§®‡•Å | ‡§ó‡§ö‡•ç‡§õ‡§§‡§ø ‚Üí boundaries at indices 4, 6\n",
    "gold = [2, 4, 7]\n",
    "predicted = [4, 6]\n",
    "\n",
    "result = compute_token_level_f1(gold, predicted)\n",
    "print(\"Token-Level Segmentation Evaluation:\")\n",
    "print(result)  # Expected: {'precision': 0.5, 'recall': 0.333, 'f1_score': 0.4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90604a0-de16-4303-b6ba-ea73b2179bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "977f029d-1a72-4311-a90b-3b674bc96701",
   "metadata": {},
   "source": [
    "## Cohen‚Äôs Kappa (Œ∫)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1c0ec3cf-b7f7-4104-9752-65125d0eb2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cohens_kappa(labels_a, labels_b):\n",
    "    \"\"\"\n",
    "    Computes Cohen's Kappa between two annotators' labels.\n",
    "\n",
    "    Args:\n",
    "        labels_a (list): Labels from annotator A\n",
    "        labels_b (list): Labels from annotator B\n",
    "\n",
    "    Returns:\n",
    "        float: Cohen's Kappa score rounded to 3 decimals\n",
    "    \"\"\"\n",
    "    if len(labels_a) != len(labels_b):\n",
    "        raise ValueError(\"Annotation lists must be of same length\")\n",
    "\n",
    "    total = len(labels_a)\n",
    "    observed_agreement = sum(a == b for a, b in zip(labels_a, labels_b)) / total\n",
    "\n",
    "    from collections import Counter\n",
    "    counter_a = Counter(labels_a)\n",
    "    counter_b = Counter(labels_b)\n",
    "\n",
    "    expected_agreement = sum((counter_a[label] / total) * (counter_b[label] / total)\n",
    "                             for label in set(counter_a) | set(counter_b))\n",
    "\n",
    "    if expected_agreement == 1:\n",
    "        return 1.0\n",
    "\n",
    "    kappa = (observed_agreement - expected_agreement) / (1 - expected_agreement)\n",
    "    return round(kappa, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "aadafa00-ce07-4e45-9b99-04187d9d1920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa: 0.615\n"
     ]
    }
   ],
   "source": [
    "labels_annotator_a = ['LOC', 'O', 'LOC', 'O', 'LOC']\n",
    "labels_annotator_b = ['LOC', 'O', 'O', 'O', 'LOC']\n",
    "\n",
    "kappa_score = compute_cohens_kappa(labels_annotator_a, labels_annotator_b)\n",
    "print(\"Cohen's Kappa:\", kappa_score)  # Expected ‚âà 0.615"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f728a3b-9c55-4b57-bf77-6688296d64cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "257d11fb-31f6-4f1b-ac1c-4d5b335b00fe",
   "metadata": {},
   "source": [
    "## Krippendorff‚Äôs Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0f274903-504d-42bf-8865-84c96444cecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cohens_kappa(labels_a, labels_b):\n",
    "    \"\"\"\n",
    "    Computes Cohen's Kappa between two annotators' labels.\n",
    "\n",
    "    Args:\n",
    "        labels_a (list): Labels from annotator A\n",
    "        labels_b (list): Labels from annotator B\n",
    "\n",
    "    Returns:\n",
    "        float: Cohen's Kappa score rounded to 3 decimals\n",
    "    \"\"\"\n",
    "    if len(labels_a) != len(labels_b):\n",
    "        raise ValueError(\"Annotation lists must be of same length\")\n",
    "\n",
    "    total = len(labels_a)\n",
    "    observed_agreement = sum(a == b for a, b in zip(labels_a, labels_b)) / total\n",
    "\n",
    "    from collections import Counter\n",
    "    counter_a = Counter(labels_a)\n",
    "    counter_b = Counter(labels_b)\n",
    "\n",
    "    expected_agreement = sum((counter_a[label] / total) * (counter_b[label] / total)\n",
    "                             for label in set(counter_a) | set(counter_b))\n",
    "\n",
    "    if expected_agreement == 1:\n",
    "        return 1.0\n",
    "\n",
    "    kappa = (observed_agreement - expected_agreement) / (1 - expected_agreement)\n",
    "    return round(kappa, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "08facf85-cfa9-4368-8cef-199c5f3600f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa: 0.615\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "labels_annotator_a = ['LOC', 'O', 'LOC', 'O', 'LOC']\n",
    "labels_annotator_b = ['LOC', 'O', 'O', 'O', 'LOC']\n",
    "\n",
    "kappa_score = compute_cohens_kappa(labels_annotator_a, labels_annotator_b)\n",
    "print(\"Cohen's Kappa:\", kappa_score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5464e2-e415-4670-901a-8dc8f725c4a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafdebdc-6b8b-4daa-835d-23784d49b4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9316c51-5252-4899-8e4e-e83157601014",
   "metadata": {},
   "source": [
    "## Informedness (Bookmaker Informedness / Youden‚Äôs J Statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "30951143-e9c4-4d24-963a-40f7495209af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_informedness(tp, tn, fp, fn):\n",
    "    \"\"\"\n",
    "    Computes Bookmaker Informedness (Youden‚Äôs J Statistic).\n",
    "\n",
    "    Args:\n",
    "        tp (int): True Positives\n",
    "        tn (int): True Negatives\n",
    "        fp (int): False Positives\n",
    "        fn (int): False Negatives\n",
    "\n",
    "    Returns:\n",
    "        float: Informedness score rounded to 3 decimals\n",
    "    \"\"\"\n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    informedness = tpr + tnr - 1\n",
    "    return round(informedness, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "67a5cdc6-25a5-4505-b11b-600df56a8d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informedness (Youden‚Äôs J): 0.583\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "TP = 3\n",
    "TN = 5\n",
    "FP = 1\n",
    "FN = 1\n",
    "\n",
    "informed_score = compute_informedness(TP, TN, FP, FN)\n",
    "print(\"Informedness (Youden‚Äôs J):\", informed_score)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3862048-8681-4bc6-8bbb-056afd17656a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d5459c3-bf03-4074-8b37-b766c8303557",
   "metadata": {},
   "source": [
    "## Matthews Correlation Coefficient (MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5ccfe07d-c3a0-496b-a200-6e9124dfccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_mcc(tp, tn, fp, fn):\n",
    "    \"\"\"\n",
    "    Computes Matthews Correlation Coefficient (MCC)\n",
    "\n",
    "    Args:\n",
    "        tp (int): True Positives\n",
    "        tn (int): True Negatives\n",
    "        fp (int): False Positives\n",
    "        fn (int): False Negatives\n",
    "\n",
    "    Returns:\n",
    "        float: MCC score rounded to 3 decimals\n",
    "    \"\"\"\n",
    "    numerator = (tp * tn) - (fp * fn)\n",
    "    denominator = math.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    if denominator == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return round(numerator / denominator, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7af4397d-bfb1-47af-be2b-41a2198470a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation Coefficient: 0.408\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "TP = 3\n",
    "TN = 4\n",
    "FP = 2\n",
    "FN = 1\n",
    "\n",
    "mcc_score = compute_mcc(TP, TN, FP, FN)\n",
    "print(\"Matthews Correlation Coefficient:\", mcc_score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86594fe4-4142-4787-a8be-9eb691a1a911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aeb5efbc-cd6e-42d1-884d-32322e2d9c60",
   "metadata": {},
   "source": [
    "## LEPOR (Length Penalty, Precision, n-gram Position difference Penalty, Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3d79ee56-be12-4a25-a389-697d5c299493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lepor(precision, recall, ref_len, hyp_len, ref_positions, hyp_positions, alpha=0.5, beta=1):\n",
    "    \"\"\"\n",
    "    Computes LEPOR score for machine translation evaluation.\n",
    "\n",
    "    Args:\n",
    "        precision (float): token precision\n",
    "        recall (float): token recall\n",
    "        ref_len (int): length of reference sentence\n",
    "        hyp_len (int): length of hypothesis sentence\n",
    "        ref_positions (list): reference token positions (1-indexed)\n",
    "        hyp_positions (list): hypothesis token positions (1-indexed, aligned to ref tokens)\n",
    "        alpha (float): weight between precision and recall (default 0.5)\n",
    "        beta (float): scaling factor (default 1)\n",
    "\n",
    "    Returns:\n",
    "        float: LEPOR score rounded to 3 decimals\n",
    "    \"\"\"\n",
    "    # Length Penalty\n",
    "    lp = min(hyp_len / ref_len, ref_len / hyp_len)\n",
    "\n",
    "    # Harmonic Mean of Precision and Recall\n",
    "    harmonic = (precision * recall) / (alpha * precision + (1 - alpha) * recall) if (precision > 0 and recall > 0) else 0.0\n",
    "\n",
    "    # Normalized Position Penalty (NPP)\n",
    "    pos_diff_sum = sum(abs(h - r) for h, r in zip(hyp_positions, ref_positions))\n",
    "    npp = pos_diff_sum / (ref_len * hyp_len) if ref_len * hyp_len > 0 else 0.0\n",
    "\n",
    "    lepor = lp * (harmonic ** beta) * (1 - npp)\n",
    "    return round(lepor, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e6c26dbd-910f-4378-9208-8b9f34e548e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEPOR Score: 0.556\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "ref_tokens = ['‡§∞‡§æ‡§Æ‡§É', '‡§µ‡§®‡§Ç', '‡§ó‡§ö‡•ç‡§õ‡§§‡§ø']\n",
    "hyp_tokens = ['‡§ó‡§ö‡•ç‡§õ‡§§‡§ø', '‡§∞‡§æ‡§Æ‡§É', '‡§µ‡§®‡§Ç']\n",
    "\n",
    "precision = recall = 1.0\n",
    "ref_len = hyp_len = 3\n",
    "ref_positions = [1, 2, 3]\n",
    "hyp_positions = [2, 3, 1]  # aligned to the same tokens in reference\n",
    "\n",
    "lepor_score = compute_lepor(precision, recall, ref_len, hyp_len, ref_positions, hyp_positions)\n",
    "print(\"LEPOR Score:\", lepor_score)  # Expected ‚âà 0.556"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6593e9d7-39bc-499a-940f-25a118b3d40b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d56d7b-e6d4-4f0f-95eb-59d7deec9d04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc27d951-72b6-4496-8251-f337f825beb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7296095b-39de-46f0-b0c8-1003bf42e036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43fe5a5-d8e6-46d5-a857-fff83c130389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b64434-4439-4a02-b6d5-17363b590dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c40aaf-c605-4e14-b2b8-6e09ef558624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e148f8-9262-4b65-bcc4-eeb939602925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e26302-332a-4784-b618-ecacc8fc7a38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c341c6ef-98c9-4b30-8e8f-d0e8b59d3ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753077c2-3a2b-4ba7-acb0-b4e92d569365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db50d5e7-cd99-4e4e-b339-30bb8633836e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
